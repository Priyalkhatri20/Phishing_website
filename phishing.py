# -*- coding: utf-8 -*-
"""phishing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XCFr0ifSR9NcRnoQwbcRzv0t6vgwg-BJ
"""

# Email Phishing Detection System for Google Colab - FIXED VERSION
# This version properly loads your Phishing_Email.csv dataset


# Step 2: Import all necessary libraries
import streamlit as st
import pandas as pd
import numpy as np
import re
import warnings
from typing import Dict, Any, Tuple
import csv
from io import StringIO
import joblib

# Machine Learning Libraries
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score
)
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

# UI Libraries
import ipywidgets as widgets
from IPython.display import display, HTML, clear_output
import matplotlib.pyplot as plt
import seaborn as sns

# Suppress warnings
warnings.filterwarnings('ignore')

# Step 3: Enhanced detector class with robust dataset loading
class EmailPhishingDetector:
    """
    Email Phishing Detection System with robust dataset loading
    """

    def __init__(self):
        self.model = None
        self.is_trained = False
        self.training_stats = {}
        self.dataset_info = {}

    def load_dataset(self, file_path: str) -> pd.DataFrame:
        """Load dataset with robust error handling"""
        try:
            print(f"📂 Loading dataset from: {file_path}")

            # First attempt: standard CSV loading
            try:
                df = pd.read_csv(file_path)
                print(f"✅ Dataset loaded successfully with standard method!")
                return df
            except pd.errors.ParserError as e:
                print(f"⚠️ CSV parsing error: {str(e)}")
                print("🔧 Trying robust parsing...")

                # Second attempt: robust parsing
                try:
                    df = pd.read_csv(
                        file_path,
                        quoting=3,  # QUOTE_NONE
                        on_bad_lines='skip',
                        engine='python',
                        encoding='utf-8',
                        low_memory=False
                    )
                    print(f"✅ Dataset loaded with robust parsing!")
                    return df
                except Exception as e2:
                    print(f"⚠️ Robust parsing failed: {str(e2)}")
                    print("🔧 Trying manual reconstruction...")
                    return self._reconstruct_csv(file_path)

        except FileNotFoundError:
            raise FileNotFoundError(f"❌ Dataset file not found: {file_path}")
        except Exception as e:
            raise Exception(f"❌ Error loading dataset: {str(e)}")

    def _reconstruct_csv(self, file_path: str) -> pd.DataFrame:
        """Manually reconstruct CSV file"""
        print("🔨 Reconstructing CSV manually...")
        rows = []
        headers = None
        skipped_rows = 0

        with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:
            # Get headers
            first_line = file.readline().strip()
            headers = [col.strip() for col in first_line.split(',')]
            print(f"📋 Headers detected: {headers}")

            # Read data
            for line_num, line in enumerate(file, 2):
                try:
                    # Simple comma split (ignore quotes)
                    row = [col.strip().strip('"') for col in line.strip().split(',')]

                    # Ensure correct number of columns
                    if len(row) >= len(headers):
                        if len(row) > len(headers):
                            # Join extra columns to last column
                            extra = ' '.join(row[len(headers)-1:])
                            row = row[:len(headers)-1] + [extra]
                        rows.append(row[:len(headers)])
                    elif len(row) == len(headers) - 1:
                        # Add empty last column if needed
                        row.append('')
                        rows.append(row)
                    else:
                        skipped_rows += 1
                        if skipped_rows <= 5:
                            print(f"⚠️ Skipping malformed row {line_num}: wrong column count")

                except Exception as e:
                    skipped_rows += 1
                    if skipped_rows <= 5:
                        print(f"⚠️ Skipping row {line_num}: {str(e)}")

        if not rows:
            raise Exception("❌ No valid rows found in CSV")

        df = pd.DataFrame(rows, columns=headers)
        print(f"✅ CSV reconstructed! Shape: {df.shape}, Skipped: {skipped_rows} rows")
        return df

    def analyze_dataset(self, df):
        """Analyze the loaded dataset"""
        print("\n📊 DATASET ANALYSIS")
        print("=" * 50)
        print(f"Dataset shape: {df.shape}")
        print(f"Columns: {list(df.columns)}")

        # Clean column names
        df.columns = df.columns.str.strip()

        # Identify text and label columns
        text_column = None
        label_column = None

        # Look for text column
        text_patterns = ['email text', 'text', 'email', 'message', 'content', 'body']
        for col in df.columns:
            col_lower = col.lower().strip()
            if any(pattern in col_lower for pattern in text_patterns):
                text_column = col
                break

        # Look for label column
        label_patterns = ['email type', 'type', 'label', 'class', 'target']
        for col in df.columns:
            col_lower = col.lower().strip()
            if any(pattern in col_lower for pattern in label_patterns):
                label_column = col
                break

        # Fallback detection
        if text_column is None:
            for col in df.columns:
                if df[col].dtype == 'object':
                    sample_length = df[col].astype(str).str.len().mean()
                    if sample_length > 50:  # Assume email text is longer
                        text_column = col
                        break

        if label_column is None:
            label_column = df.columns[-1]  # Assume last column is label

        print(f"📧 Text column: '{text_column}'")
        print(f"🏷️ Label column: '{label_column}'")

        if text_column and label_column:
            # Show class distribution
            print(f"\n📈 Class distribution:")
            print(df[label_column].value_counts())

            # Show sample data
            print(f"\n📄 Sample data:")
            for i in range(min(3, len(df))):
                text_sample = str(df[text_column].iloc[i])[:100] + "..." if len(str(df[text_column].iloc[i])) > 100 else str(df[text_column].iloc[i])
                print(f"Row {i+1}: {df[label_column].iloc[i]} - {text_sample}")

        self.dataset_info = {
            'text_column': text_column,
            'label_column': label_column,
            'shape': df.shape,
            'columns': list(df.columns)
        }

        return text_column, label_column

    def extract_email_features(self, text: str) -> Dict[str, float]:
        """Extract comprehensive features from email text"""
        if pd.isna(text) or not isinstance(text, str):
            text = ""

        text = str(text).lower()  # Convert to lowercase for analysis
        features = {}

        # Basic text features
        features['email_length'] = len(text)
        features['word_count'] = len(text.split())
        features['sentence_count'] = len(re.findall(r'[.!?]+', text))
        features['avg_word_length'] = np.mean([len(word) for word in text.split()]) if text.split() else 0

        # URL features
        urls = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text)
        features['url_count'] = len(urls)

        # Suspicious URL indicators
        suspicious_domains = ['bit.ly', 'tinyurl', 't.co', 'goo.gl', 'ow.ly', 'short.link']
        features['has_suspicious_url'] = int(any(domain in text for domain in suspicious_domains))

        # Phishing keywords (comprehensive list)
        phishing_keywords = [
            'urgent', 'immediate', 'verify', 'suspend', 'suspended', 'click here', 'act now',
            'confirm', 'update', 'expire', 'expires', 'expiring', 'limited time', 'winner',
            'congratulations', 'free', 'prize', 'lottery', 'inheritance', 'security alert',
            'account locked', 'unauthorized', 'fraud', 'refund', 'claim now', 'act fast',
            'final notice', 'last chance', 'opportunity', 'selected', 'chosen', 'million',
            'thousand', 'dollars', 'pounds', 'euros', 'payment', 'transaction', 'billing'
        ]
        features['suspicious_word_count'] = sum(text.count(word) for word in phishing_keywords)

        # Financial and banking terms
        financial_terms = [
            'bank', 'account', 'credit', 'debit', 'payment', 'money', 'transfer', 'paypal',
            'visa', 'mastercard', 'transaction', 'billing', 'invoice', 'charge', 'fee'
        ]
        features['financial_terms'] = sum(text.count(term) for term in financial_terms)

        # Urgency indicators
        urgency_terms = ['asap', 'immediately', 'urgent', 'hurry', 'quick', 'fast', 'now', 'today']
        features['urgency_count'] = sum(text.count(term) for term in urgency_terms)

        # Capitalization and punctuation
        features['caps_ratio'] = len(re.findall(r'[A-Z]', str(text))) / max(len(str(text)), 1)
        features['exclamation_count'] = text.count('!')
        features['question_count'] = text.count('?')

        # Email and phone patterns
        features['email_mentions'] = len(re.findall(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', text))
        features['phone_mentions'] = len(re.findall(r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b', text))

        # Suspicious patterns
        features['has_click_here'] = int('click here' in text or 'click now' in text)
        features['has_verify_account'] = int('verify' in text and 'account' in text)
        features['has_update_info'] = int(('update' in text and 'information' in text) or ('update' in text and 'details' in text))

        return features

    def preprocess_data(self, df, text_column, label_column):
        """Preprocess dataset for training"""
        print(f"\n🔧 PREPROCESSING DATA")
        print("=" * 30)

        # Clean text data
        df[text_column] = df[text_column].fillna('').astype(str)

        # Extract features
        print("🔍 Extracting features...")
        feature_dicts = df[text_column].apply(self.extract_email_features)
        features = pd.DataFrame(list(feature_dicts))

        # Prepare target variable
        target_series = df[label_column]
        unique_values = target_series.unique()
        print(f"📊 Unique target values: {unique_values}")

        # Convert target to binary
        if len(unique_values) == 2:
            # Handle different label formats
            if 'phishing' in str(unique_values).lower():
                # Find which value represents phishing
                phishing_label = [val for val in unique_values if 'phishing' in str(val).lower()][0]
                target = (target_series == phishing_label).astype(int)
            elif 'safe' in str(unique_values).lower():
                # Find which value represents safe
                safe_label = [val for val in unique_values if 'safe' in str(val).lower()][0]
                target = (target_series != safe_label).astype(int)
            else:
                # Default: second unique value is phishing
                target = (target_series == unique_values[1]).astype(int)
        else:
            raise ValueError(f"Expected binary classification, got {len(unique_values)} classes: {unique_values}")

        print(f"✅ Features extracted: {features.shape}")
        print(f"📈 Target distribution: {target.value_counts().to_dict()}")

        return features, target

    def train_model(self, features, target):
        """Train multiple models and select the best"""
        print(f"\n🤖 TRAINING MODELS")
        print("=" * 30)

        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            features, target, test_size=0.2, random_state=42, stratify=target
        )

        print(f"📊 Training: {X_train.shape}, Testing: {X_test.shape}")

        # Define models with better parameters
        models = {
            'Random Forest': RandomForestClassifier(
                n_estimators=200,
                max_depth=15,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=42,
                class_weight='balanced'
            ),
            'Logistic Regression': LogisticRegression(
                random_state=42,
                class_weight='balanced',
                max_iter=2000,
                C=1.0
            )
        }

        best_model = None
        best_score = 0
        results = {}

        # Train and evaluate models
        for name, model in models.items():
            print(f"\n🏋️ Training {name}...")

            # Create pipeline
            pipeline = Pipeline([
                ('scaler', StandardScaler()),
                ('classifier', model)
            ])

            # Train model
            pipeline.fit(X_train, y_train)

            # Predictions
            y_pred = pipeline.predict(X_test)
            y_pred_proba = pipeline.predict_proba(X_test)[:, 1]

            # Calculate metrics
            accuracy = accuracy_score(y_test, y_pred)
            precision = precision_score(y_test, y_pred, zero_division=0)
            recall = recall_score(y_test, y_pred, zero_division=0)
            f1 = f1_score(y_test, y_pred, zero_division=0)

            # Cross-validation
            cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='f1')

            results[name] = {
                'model': pipeline,
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                'cv_mean': cv_scores.mean(),
                'cv_std': cv_scores.std(),
                'y_pred': y_pred,
                'y_test': y_test
            }

            print(f"   Accuracy: {accuracy:.3f}")
            print(f"   Precision: {precision:.3f}")
            print(f"   Recall: {recall:.3f}")
            print(f"   F1-Score: {f1:.3f}")
            print(f"   CV Score: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}")

            # Select best model
            if f1 > best_score:
                best_score = f1
                best_model = name

        # Set best model
        self.model = results[best_model]['model']
        self.is_trained = True
        self.training_stats = results[best_model]

        print(f"\n🏆 BEST MODEL: {best_model}")
        print(f"   F1-Score: {best_score:.3f}")

        # Display confusion matrix
        self.display_results(results, best_model)

        return results

    def display_results(self, results, best_model):
        """Display training results"""
        print(f"\n📊 MODEL EVALUATION")
        print("=" * 40)

        # Performance comparison
        perf_df = pd.DataFrame(results).T
        print(perf_df[['accuracy', 'precision', 'recall', 'f1_score']].round(3))

        # Confusion Matrix for best model
        y_test = results[best_model]['y_test']
        y_pred = results[best_model]['y_pred']

        plt.figure(figsize=(12, 4))

        # Confusion Matrix
        plt.subplot(1, 3, 1)
        cm = confusion_matrix(y_test, y_pred)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=['Safe', 'Phishing'],
                   yticklabels=['Safe', 'Phishing'])
        plt.title(f'Confusion Matrix\n{best_model}')
        plt.ylabel('Actual')
        plt.xlabel('Predicted')

        # Performance metrics
        plt.subplot(1, 3, 2)
        metrics = ['accuracy', 'precision', 'recall', 'f1_score']
        values = [results[best_model][metric] for metric in metrics]
        bars = plt.bar(metrics, values, color=['skyblue', 'lightgreen', 'orange', 'pink'])
        plt.title('Performance Metrics')
        plt.ylabel('Score')
        plt.ylim(0, 1)
        for bar, value in zip(bars, values):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{value:.3f}', ha='center', va='bottom')

        # Model comparison
        plt.subplot(1, 3, 3)
        model_names = list(results.keys())
        f1_scores = [results[name]['f1_score'] for name in model_names]
        bars = plt.bar(model_names, f1_scores, color=['lightblue', 'lightcoral'])
        plt.title('Model Comparison (F1-Score)')
        plt.ylabel('F1-Score')
        plt.ylim(0, 1)
        for bar, score in zip(bars, f1_scores):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{score:.3f}', ha='center', va='bottom')

        plt.tight_layout()
        plt.show()

    def predict_email(self, email_text):
        """Predict if email is phishing"""
        if not self.is_trained:
            return {"error": "Model not trained yet!"}

        # Extract features
        features = self.extract_email_features(email_text)
        feature_df = pd.DataFrame([features])

        # Make prediction
        prediction = self.model.predict(feature_df)[0]
        probabilities = self.model.predict_proba(feature_df)[0]

        confidence = max(probabilities)

        result = {
            'prediction': 'Phishing Email' if prediction == 1 else 'Safe Email',
            'probability_safe': probabilities[0],
            'probability_phishing': probabilities[1],
            'confidence': confidence,
            'risk_level': 'High' if confidence > 0.8 else 'Medium' if confidence > 0.6 else 'Low',
            'features': features
        }

        return result

# Step 4: Load and train with your dataset
print("🚀 INITIALIZING EMAIL PHISHING DETECTOR")
print("=" * 50)

detector = EmailPhishingDetector()

# Try to load your dataset
dataset_found = False
try:
    # Load your actual dataset
    df = detector.load_dataset('Phishing_Email.csv')

    # Analyze dataset
    text_col, label_col = detector.analyze_dataset(df)

    if text_col and label_col:
        # Preprocess data
        features, target = detector.preprocess_data(df, text_col, label_col)

        # Train model
        results = detector.train_model(features, target)
        dataset_found = True

        print("\n✅ SUCCESS!")
        print("=" * 20)
        print(f"Dataset loaded: {df.shape}")
        print(f"Model trained: {detector.training_stats['f1_score']:.3f} F1-score")
        print(f"Ready for predictions!")

    else:
        raise ValueError("Could not identify text and label columns")

except Exception as e:
    print(f"\n❌ ERROR LOADING DATASET: {str(e)}")
    print("\n💡 TROUBLESHOOTING:")
    print("1. Make sure 'Phishing_Email.csv' is uploaded to Colab")
    print("2. Check if the file has the correct format")
    print("3. Upload your dataset using the file browser on the left")
    dataset_found = False

# Step 5: Create UI
def create_prediction_ui():
    """Create the prediction interface"""

    style = {'description_width': 'initial'}

    # Input area
    email_input = widgets.Textarea(
        value='',
        placeholder='Enter your email content here to check if it\'s phishing or safe...',
        description='📧 Email Text:',
        rows=10,
        style=style,
        layout=widgets.Layout(width='100%', height='200px')
    )

    # Buttons
    analyze_btn = widgets.Button(
        description='🔍 Analyze Email',
        button_style='primary',
        layout=widgets.Layout(width='150px', height='40px')
    )

    clear_btn = widgets.Button(
        description='🗑️ Clear',
        button_style='warning',
        layout=widgets.Layout(width='100px', height='40px')
    )

    # Example selector
    examples = {
        'Select an example...': '',
        '🚨 Phishing Example 1': 'URGENT ACTION REQUIRED! Your bank account has been temporarily suspended due to suspicious activity. To reactivate your account immediately, please click here and verify your login credentials: http://bit.ly/bank-verify-urgent. Failure to act within 24 hours will result in permanent account closure. This is a final notice.',
        '🚨 Phishing Example 2': 'Congratulations! You have been selected as the winner of our $500,000 international email lottery! To claim your prize money, please reply with your full name, address, phone number, and bank account details. This is a limited time offer that expires in 48 hours. Click here to claim: http://lottery-winner.co/claim',
        '🚨 Phishing Example 3': 'SECURITY ALERT: We have detected unauthorized access to your PayPal account from an unknown device. Your account has been temporarily limited for your protection. Please verify your identity immediately by clicking the link below: http://paypal-security.net/verify. If you do not verify within 6 hours, your account will be permanently suspended.',
        '✅ Safe Example 1': 'Hi Sarah, I hope you are doing well. I wanted to follow up on our meeting yesterday about the quarterly marketing campaign. Could you please send me the budget breakdown we discussed? I need to review it before presenting to the board next week. Thanks for your time and looking forward to your response. Best regards, John',
        '✅ Safe Example 2': 'Dear Customer, Thank you for your recent order #ORD-789123. Your purchase of the wireless headphones has been confirmed and will be shipped within 2-3 business days. You will receive a tracking number via email once your order has been dispatched. If you have any questions, please contact our customer service team.',
        '✅ Safe Example 3': 'Hello team, I hope everyone had a great weekend. Just a reminder that we have our monthly team meeting scheduled for tomorrow at 10 AM in the conference room. We will be discussing the progress on current projects and planning for next quarter. Please bring your project status reports. See you all tomorrow!'
    }

    example_dropdown = widgets.Dropdown(
        options=examples,
        description='📝 Try Examples:',
        style=style,
        layout=widgets.Layout(width='400px')
    )

    # Output area
    output = widgets.Output()

    # Event handlers
    def on_analyze(b):
        with output:
            clear_output(wait=True)

            if not dataset_found:
                display(HTML("""
                <div style='background-color: #f8d7da; color: #721c24; padding: 15px; border-radius: 5px; margin: 10px 0;'>
                    <h3>❌ Dataset Not Available</h3>
                    <p>Please upload your 'Phishing_Email.csv' file to train the model.</p>
                    <p>Use the file browser on the left to upload your dataset.</p>
                </div>
                """))
                return

            email_text = email_input.value.strip()
            if not email_text:
                display(HTML("""
                <div style='background-color: #fff3cd; color: #856404; padding: 10px; border-radius: 5px;'>
                    ⚠️ Please enter an email to analyze!
                </div>
                """))
                return

            print("🔍 Analyzing email...")
            result = detector.predict_email(email_text)

            if 'error' in result:
                display(HTML(f"""
                <div style='background-color: #f8d7da; color: #721c24; padding: 10px; border-radius: 5px;'>
                    ❌ {result['error']}
                </div>
                """))
                return

            # Display prediction result
            is_phishing = result['prediction'] == 'Phishing Email'
            bg_color = '#f8d7da' if is_phishing else '#d4edda'
            text_color = '#721c24' if is_phishing else '#155724'
            icon = '🚨' if is_phishing else '✅'

            display(HTML(f"""
            <div style='background-color: {bg_color}; color: {text_color}; padding: 20px; border-radius: 10px; margin: 15px 0; border-left: 5px solid {"#dc3545" if is_phishing else "#28a745"}'>
                <h2 style='margin-top: 0;'>{icon} {result['prediction'].upper()}</h2>
                <div style='display: grid; grid-template-columns: 1fr 1fr; gap: 15px; margin: 15px 0;'>
                    <div><strong>Confidence:</strong> {result['confidence']:.1%}</div>
                    <div><strong>Risk Level:</strong> {result['risk_level']}</div>
                    <div><strong>Phishing Probability:</strong> {result['probability_phishing']:.1%}</div>
                    <div><strong>Safe Probability:</strong> {result['probability_safe']:.1%}</div>
                </div>
            </div>
            """))

            # Security recommendations
            if is_phishing:
                display(HTML("""
                <div style='background-color: #fff3cd; border: 1px solid #ffeaa7; padding: 15px; border-radius: 5px; margin: 10px 0;'>
                    <h3 style='color: #856404; margin-top: 0;'>🛡️ SECURITY RECOMMENDATIONS</h3>
                    <ul style='color: #856404; margin: 10px 0;'>
                        <li><strong>DO NOT</strong> click any links in this email</li>
                        <li><strong>DO NOT</strong> download attachments</li>
                        <li><strong>DO NOT</strong> provide personal information</li>
                        <li><strong>VERIFY</strong> sender through official channels</li>
                        <li><strong>REPORT</strong> to your IT security team</li>
                        <li><strong>DELETE</strong> the email immediately</li>
                    </ul>
                </div>
                """))
            else:
                if result['confidence'] < 0.7:
                    display(HTML("""
                    <div style='background-color: #d1ecf1; border: 1px solid #bee5eb; padding: 15px; border-radius: 5px; margin: 10px 0;'>
                        <h3 style='color: #0c5460; margin-top: 0;'>💡 MODERATE CONFIDENCE</h3>
                        <p style='color: #0c5460; margin: 10px 0;'>While this email appears safe, always verify sender identity for sensitive requests.</p>
                    </div>
                    """))
                else:
                    display(HTML("""
                    <div style='background-color: #d4edda; border: 1px solid #c3e6cb; padding: 15px; border-radius: 5px; margin: 10px 0;'>
                        <h3 style='color: #155724; margin-top: 0;'>✅ EMAIL APPEARS SAFE</h3>
                        <p style='color: #155724; margin: 10px 0;'>This email shows strong indicators of being legitimate.</p>
                    </div>
                    """))

            # Feature analysis
            print("\n🔍 FEATURE ANALYSIS:")
            print("-" * 30)
            key_features = ['suspicious_word_count', 'url_count', 'exclamation_count',
                          'financial_terms', 'urgency_count', 'has_click_here']

            for feature in key_features:
                if feature in result['features']:
                    print(f"- {feature.replace('_', ' ').title()}: {result['features'][feature]}")


    def on_clear(b):
        with output:
            clear_output(wait=True)
            email_input.value = ''
            example_dropdown.value = ''

    def on_example_change(change):
        email_input.value = change['new']

    analyze_btn.on_click(on_analyze)
    clear_btn.on_click(on_clear)
    example_dropdown.observe(on_example_change, names='value')

    # Layout
    buttons_box = widgets.HBox([analyze_btn, clear_btn])
    ui = widgets.VBox([email_input, example_dropdown, buttons_box, output])
    display(ui)

# Step 6: Display the UI if dataset is found
if dataset_found:
    create_prediction_ui()
else:
    print("\nUI not created because dataset was not loaded successfully.")

import joblib
joblib.dump(detector.model, "phishing_model.pkl")
# Later
model = joblib.load("phishing_model.pkl")
